{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d37d76c-4736-47e0-bdf3-924b4757949d",
   "metadata": {},
   "source": [
    "Линейная регрессия (Linear regression) — один из простейший алгоритмов машинного обучения, описывающий зависимость целевой переменной от признака в виде линейной функции y = kx + b. В данном случае была представлена простая или парная линейная регрессия, а уравнение вида f_{w, b}(x) = w_0x_0 + w_1x_1 +... + w_{n}x_{n} + b = w \\cdot x + b называется множественной линейной регрессией, где b — смещение модели, w — вектор её весов, а x — вектор признаков одного обучающего образца.\n",
    "\n",
    "К другим условиям определения линейной регрессии относятся гомоскедастичность (дисперсия остатков постоянна и конечна), а также отсутствие мильтиколлинеарности (линейной зависимости между признаками)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "571ac198-b998-4ac5-bf26-b3f350938339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale, PolynomialFeatures\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf06870b-665f-4ae6-b42e-d61064415749",
   "metadata": {},
   "source": [
    "### Аналитическое решение в виде w = (X_T * X)**-1 * X_T * y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5871bd94-c893-454c-842b-115a73748611",
   "metadata": {},
   "source": [
    "Где w - веса(коэфициенты) признаков x, w0 - смещение(свободный член) который сдвигает нашу линию по y, иначе у нас бы всегда линия начиналась бы с нуля"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "79196fa8-7b0b-419f-8bd0-7f12d2d63b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixLinearRegression:\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.insert(X, 0, 1, axis=1) #Добавляем столбец с 1-ми в нашу матрицу чтобы учесть bias(w0 при y = w0 + w1*x.....)\n",
    "        XT_X_inv = np.linalg.inv(X.T @ X) #находим (X * X_T)**-1 чтобы подстваить в w = (X * X_T)**-1 * X_T * y\n",
    "        weights = np.linalg.multi_dot([XT_X_inv, X.T, y])  #находим сами веса w через матричное умножение\n",
    "        self.bias, self.weights = weights[0], weights[1:] #Присваиваем нулевой столбец в bias(w0), а остальные в веса признаков \n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return X_test @ self.weights + self.bias #Возвращаем y = X * w + w0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a6a175-7945-4d2f-8008-3a9eeafb801c",
   "metadata": {},
   "source": [
    "Как работает наше аналитическое решение под капотом?\n",
    "Наш метод тупо берёт всю матрицу данных и вычисляет единственное оптимальное решение с помощью формулы нормального уравнения:\n",
    "1)Без градиентного спуска, без обновления весов, без итераций.\n",
    "2)Без функций потерь, потому что формула сразу выдаёт оптимальные веса, минимизирующие среднеквадратичную ошибку (MSE).\n",
    "3) Это просто линейная алгебра: перемножение матриц, обращение, и вуаля — у нас есть веса!\n",
    "\n",
    "Почему это усреднённое решение?\n",
    "Линейная регрессия ищет прямую, которая максимально приближает данные, минимизируя среднеквадратичное отклонение (MSE).\n",
    "\n",
    "То есть:\n",
    "Берём все точки (матрицу X)\n",
    "Считаем общее направление их зависимости от y (перемножаем, инвертируем).\n",
    "Выводим единственный набор коэффициентов w, который даёт оптимальную прямую.\n",
    "Это похоже на усреднение данных в пространстве n-мерных признаков.\n",
    "\n",
    "Недостатки этого метода:\n",
    "1) Высокая вычислительная сложность\n",
    "2) Плохо работает с мультиколлинеарностью:\n",
    "    Если признаки сильно коррелируют, X может стать необратимой (сингулярной) матрицей.\n",
    "    В таком случае матрица не инвертируется, и код сломается.\n",
    "    Решение: использовать регуляризацию (L2-регрессия, Ridge)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490c9a3b-1964-4a9f-b06a-755bc4da0704",
   "metadata": {},
   "source": [
    "### Решение при помощи Градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab8f74-412f-40c7-bb64-25a5dc8f08dc",
   "metadata": {},
   "source": [
    "Градиентный спуск — это итеративный метод оптимизации, \n",
    "который помогает найти минимум функции потерь путем обновления весов модели в направлении отрицательного градиента."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e31ad13-78b2-4f17-b7b9-b621a613ad8b",
   "metadata": {},
   "source": [
    "Где learning_rate - шаги которые дают нам двиагться вперед а точнее умножая эти шаги мы ищем идеальные веса.\n",
    "learning_rate определяет, насколько сильно мы корректируем наши веса на каждом шаге.\n",
    "Если шаг слишком большой, мы можем \"перепрыгнуть\" минимум и не сойтись. Если шаг слишком маленький, алгоритм будет сходиться очень медленно.\n",
    "То есть, каждый раз мы делаем шаг в сторону минимума, но не \"прыгаем\" слишком далеко, чтобы не пропустить его."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "33153401-3063-4c8b-916e-b84b6c9c1689",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDLinearRegression:\n",
    "    def __init__(self, learning_rate=0.01, tolerance=1e-8): #устанавливаем нужные значения learning_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tolerance = tolerance\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape #n_samples = len of rows, n_features = num of columns\n",
    "        self.bias, self.weights = 0, np.zeros(n_features) # устанавливаем начальные веса\n",
    "        previous_db, previous_dw = 0, np.zeros(n_features) # устанавливаем начальные градиенты\n",
    "        \n",
    "        while True:\n",
    "            y_pred = X @ self.weights + self.bias #Делаем прогноз с начальными значениями\n",
    "            db = 1 / n_samples * np.sum(y_pred - y) #Ищем градиент по bias-у\n",
    "            dw = 1 / n_samples * X.T @ (y_pred - y) #Ищем градиаент по весу w\n",
    "\n",
    "            self.weights -= self.learning_rate * dw #Обновляем веса умножая на наш \"шаг\"\n",
    "            self.bias -= self.learning_rate * db # Обновляем bias умножая на наш \"шаг\"\n",
    "\n",
    "            #Находим разницу прошлого и нынешнего градиента чтобы при случае минимальног различие в ту или иную сторону остановить обучение ведь достигается минимум\n",
    "            diff_db = np.abs(previous_db - db) \n",
    "            diff_dw = np.abs(previous_dw - dw)\n",
    "\n",
    "            #Кусок кода который остановит обучения при приюлижения к минимуму \n",
    "            if diff_db < self.tolerance:\n",
    "                if diff_dw.all() < self.tolerance:\n",
    "                    break\n",
    "            previous_db = db\n",
    "            previous_dw = dw       \n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return X_test @ self.weights + self.bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943bdf49-3ecf-4133-b4fd-1e0b9be96b6a",
   "metadata": {},
   "source": [
    "Вообще как работает градиентный спуск здесь:\n",
    "Мое обьяснение: Здесь изначально bias и веса у нас заданы как ноль, после чего мы делаем сравнение(прогноз), на основе этого прогноза пересчитывается \n",
    "наши веса и тд и тп шагая маленькиими шагами а именно как мы задали в learning_rate, а каким образом? \n",
    "Каждый шаг мы подставляем значение в наш градиент(направление нашего минимума) и обновляем наши веса и обновляем до того момента пока наша \n",
    "разница не будет такой что меньше заданного tolerance что будет значить мы подобрались к минимуму и это наш ответ.\n",
    "\n",
    "Помощь гпт:\n",
    "Градиентный спуск в этом коде работает следующим образом: сначала мы инициализируем все веса и bias значением 0, \n",
    "а затем итеративно обновляем их, минимизируя ошибку предсказания.\n",
    "В каждой итерации мы сначала вычисляем прогнозируемые значения y_pred с текущими весами, \n",
    "затем находим разницу между предсказанными и реальными значениями y, которая используется для вычисления градиентов db и dw. \n",
    "Эти градиенты показывают, в каком направлении и насколько сильно нужно скорректировать bias и веса, чтобы уменьшить ошибку.\n",
    "\n",
    "Обновление параметров происходит с использованием learning rate — небольшого шага, \n",
    "который регулирует, насколько сильно изменяются bias и веса в сторону минимума.\n",
    "После каждого обновления проверяется, насколько сильно изменились db и dw по сравнению с предыдущими значениями. \n",
    "Если эти изменения становятся меньше заданного tolerance, алгоритм останавливается, так как дальнейшее обновление не приведёт к значительному улучшению.\n",
    "\n",
    "Таким образом, градиентный спуск постепенно подбирает такие значения bias и весов,\n",
    "при которых разница между предсказанными и реальными значениями минимальна, \n",
    "и алгоритм сходится к оптимальному решению."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d933aa0-8562-450d-b014-86805efac79b",
   "metadata": {},
   "source": [
    "1. Возможные проблемы и ошибки в градиентном спуске\n",
    "1.1. Неправильный learning rate (шаг градиентного спуска)\n",
    "Слишком маленький learning rate\n",
    "\n",
    "Обучение слишком долгое (модель делает крошечные шаги).\n",
    "Можно застрять в локальном минимуме или не достичь глобального минимума.\n",
    "Решение: увеличить learning rate, но аккуратно.\n",
    "Слишком большой learning rate\n",
    "\n",
    "Модель может \"перепрыгивать\" минимум и не сойтись.\n",
    "Ошибка может скакать хаотично.\n",
    "Возможна расходимость — веса становятся огромными, и ошибка увеличивается бесконечно.\n",
    "Решение: уменьшить learning rate или использовать затухающую скорость обучения (learning rate decay).\n",
    "1.2. Неправильное направление градиента\n",
    "Градиент указывает неверное направление, если:\n",
    "Ошибка в вычислении производных.\n",
    "В данных есть аномалии (выбросы), сильно влияющие на градиент.\n",
    "Используется неподходящая функция потерь.\n",
    "Пример ошибки:\n",
    "Если использовать градиентный спуск без нормализации данных, градиенты могут быть слишком разными (один вес обновляется быстро, другой медленно), что приводит к медленной или нестабильной сходимости.\n",
    "\n",
    "Решение:\n",
    "\n",
    "Нормализовать данные (привести признаки к единому масштабу, например, StandardScaler).\n",
    "Проверить правильность формул градиентов.\n",
    "1.3. Локальные минимумы и седловые точки\n",
    "Если функция потерь имеет несколько минимумов, градиентный спуск может застрять в локальном минимуме.\n",
    "Если градиент почти нулевой во всех направлениях, можно попасть в седловую точку (точка, где градиент нулевой, но это не минимум).\n",
    "Решение:\n",
    "\n",
    "Попробовать другой начальный набор весов (инициализация весов случайными значениями).\n",
    "Использовать разные методы градиентного спуска, например, Momentum, Adam, RMSprop, которые помогают избежать таких проблем.\n",
    "1.4. Веса модели уходят в NaN или бесконечность\n",
    "Такое бывает, если:\n",
    "Слишком большой learning rate — градиенты становятся огромными.\n",
    "Деление на 0 в вычислениях (например, из-за логарифма).\n",
    "В данных есть NaN или бесконечные значения.\n",
    "Решение:\n",
    "Проверить данные перед обучением (np.isnan().sum(), np.isinf().sum()).\n",
    "Уменьшить learning rate.\n",
    "Использовать градиентный спуск с моментом (Momentum)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0376305c-9f6c-4a46-bdd0-7470752cb9fc",
   "metadata": {},
   "source": [
    "### Обучение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "354def8c-ff07-489e-8bb0-2de1ddb44acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    age  experience  income\n",
      "0    25           1   30450\n",
      "1    30           3   35670\n",
      "2    47           2   31580\n",
      "3    32           5   40130\n",
      "4    43          10   47830\n",
      "5    51           7   41630\n",
      "6    28           5   41340\n",
      "7    33           4   37650\n",
      "8    37           5   40250\n",
      "9    39           8   45150\n",
      "10   29           1   27840\n",
      "11   47           9   46110\n",
      "12   54           5   36720\n",
      "13   51           4   34800\n",
      "14   44          12   51300\n",
      "15   41           6   38900\n",
      "16   58          17   63600\n",
      "17   23           1   30870\n",
      "18   44           9   44190\n",
      "19   37          10   48700\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e8dfb_row0_col0, #T_e8dfb_row1_col1, #T_e8dfb_row2_col2 {\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8dfb_row0_col1, #T_e8dfb_row0_col2, #T_e8dfb_row2_col0 {\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8dfb_row1_col0 {\n",
       "  background-color: #7396f5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8dfb_row1_col2 {\n",
       "  background-color: #c0282f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8dfb_row2_col1 {\n",
       "  background-color: #c32e31;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e8dfb\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e8dfb_level0_col0\" class=\"col_heading level0 col0\" >age</th>\n",
       "      <th id=\"T_e8dfb_level0_col1\" class=\"col_heading level0 col1\" >experience</th>\n",
       "      <th id=\"T_e8dfb_level0_col2\" class=\"col_heading level0 col2\" >income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e8dfb_level0_row0\" class=\"row_heading level0 row0\" >age</th>\n",
       "      <td id=\"T_e8dfb_row0_col0\" class=\"data row0 col0\" >1.000000</td>\n",
       "      <td id=\"T_e8dfb_row0_col1\" class=\"data row0 col1\" >0.615165</td>\n",
       "      <td id=\"T_e8dfb_row0_col2\" class=\"data row0 col2\" >0.532204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8dfb_level0_row1\" class=\"row_heading level0 row1\" >experience</th>\n",
       "      <td id=\"T_e8dfb_row1_col0\" class=\"data row1 col0\" >0.615165</td>\n",
       "      <td id=\"T_e8dfb_row1_col1\" class=\"data row1 col1\" >1.000000</td>\n",
       "      <td id=\"T_e8dfb_row1_col2\" class=\"data row1 col2\" >0.984227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8dfb_level0_row2\" class=\"row_heading level0 row2\" >income</th>\n",
       "      <td id=\"T_e8dfb_row2_col0\" class=\"data row2 col0\" >0.532204</td>\n",
       "      <td id=\"T_e8dfb_row2_col1\" class=\"data row2 col1\" >0.984227</td>\n",
       "      <td id=\"T_e8dfb_row2_col2\" class=\"data row2 col2\" >1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x19fdd499cd0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('multiple_linear_regression_dataset.csv')\n",
    "X, y = df.iloc[:, :-1].values, df.iloc[:, -1].values\n",
    "X_scaled = scale(X1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "X_train_s, X_test_s, y_train, y_test = train_test_split(X_scaled, y, random_state=0)\n",
    "print(df)\n",
    "\n",
    "correlation_matrix = df.corr()\n",
    "correlation_matrix.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce9d274-809a-4c4e-926a-3119ee0b205c",
   "metadata": {},
   "source": [
    "#### Matrix Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "efb027a9-96e7-41df-b6b7-7876bc3fc075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Linear regression  R2 score: 0.9307237996010834\n",
      "Matrix Linear regression MAPE: 0.04666577176525873 \n",
      "\n",
      "weights: (40922.38666080791, -1049.7866043333488, 8718.764356365164)\n",
      "prediction: [46528.00800666 35018.47848628 49448.73803373 38604.36954966\n",
      " 30788.13913983]\n"
     ]
    }
   ],
   "source": [
    "inear_regression = MatrixLinearRegression()\n",
    "linear_regression.fit(X_train_s, y_train)\n",
    "pred_res = matrix_linear_regression.predict(X_test_s)\n",
    "r2 = r2_score(y_test, matrix_lr_pred_res)\n",
    "mape = mean_absolute_percentage_error(y_test, matrix_lr_pred_res)\n",
    "\n",
    "print(f'Matrix Linear regression  R2 score: {r2}')\n",
    "print(f'Matrix Linear regression MAPE: {mape}', '\\n')\n",
    "\n",
    "print(f'weights: {linear_regression.bias, *linear_regression.weights}')\n",
    "print(f'prediction: {pred_res}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f197513b-1858-4ed5-aae4-22d6fa9ab648",
   "metadata": {},
   "source": [
    "#### GD Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "74772733-908f-4d73-b81d-f6f3cfd5b329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression R2 score: 0.9307237996011029\n",
      "Linear regression MAPE: 0.04666577176525262 \n",
      "\n",
      "weights: (40922.38666080791, -1049.7866043333488, 8718.764356365164)\n",
      "prediction: [46528.00800666 35018.47848628 49448.73803373 38604.36954966\n",
      " 30788.13913983]\n"
     ]
    }
   ],
   "source": [
    "linear_regression = GDLinearRegression()\n",
    "linear_regression.fit(X_train_s, y_train)\n",
    "pred_res = linear_regression.predict(X_test_s)\n",
    "r2 = r2_score(y_test, pred_res)\n",
    "mape = mean_absolute_percentage_error(y_test, pred_res)\n",
    "\n",
    "print(f'Linear regression R2 score: {r2}')\n",
    "print(f'Linear regression MAPE: {mape}', '\\n')\n",
    "\n",
    "print(f'weights: {linear_regression.bias, *linear_regression.weights}')\n",
    "print(f'prediction: {pred_res}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dc7550-c663-4bf1-b53b-fe8ff8f7e8b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
